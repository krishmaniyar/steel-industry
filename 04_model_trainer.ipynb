{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Trainer Notebook\n",
    "## Steel Industry Load Type Prediction\n",
    "\n",
    "This notebook handles the model training phase of our ML pipeline.\n",
    "\n",
    "**Objectives:**\n",
    "- Load the transformed data\n",
    "- Split data into training and testing sets\n",
    "- Train multiple machine learning models\n",
    "- Evaluate and compare model performance\n",
    "- Perform hyperparameter tuning\n",
    "- Save the best performing model\n",
    "- Generate comprehensive evaluation reports"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T04:33:31.906938Z",
     "start_time": "2025-10-13T04:33:31.686597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Additional ML libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Model Training Phase Started\")\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "print(f\"XGBoost available: {XGBOOST_AVAILABLE}\")\n",
    "print(f\"LightGBM available: {LIGHTGBM_AVAILABLE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Phase Started\n",
      "Timestamp: 2025-10-13 10:03:31.905605\n",
      "XGBoost available: False\n",
      "LightGBM available: False\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Load Transformed Data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T04:33:32.154004Z",
     "start_time": "2025-10-13T04:33:31.916064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the transformed data\n",
    "input_filename = 'transformed_data.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_filename)\n",
    "    print(f\"✓ Data loaded successfully from {input_filename}\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: {input_filename} not found. Please run 03_data_transformation.ipynb first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    raise"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded successfully from transformed_data.csv\n",
      "Dataset shape: (35040, 38)\n",
      "Memory usage: 9.78 MB\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T04:33:32.167093Z",
     "start_time": "2025-10-13T04:33:32.162847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load additional artifacts\n",
    "try:\n",
    "    # Load encoders and metadata\n",
    "    encoders = joblib.load('encoders.pkl')\n",
    "    feature_names = joblib.load('feature_names.pkl')\n",
    "    transformation_metadata = joblib.load('transformation_metadata.pkl')\n",
    "\n",
    "    print(\"✓ Additional artifacts loaded successfully\")\n",
    "    print(f\"Feature names: {len(feature_names)} features\")\n",
    "    print(f\"Target classes: {transformation_metadata['target_classes']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Warning: Could not load additional artifacts: {e}\")\n",
    "    feature_names = [col for col in df.columns if col != 'target']"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Additional artifacts loaded successfully\n",
      "Feature names: 37 features\n",
      "Target classes: {'Light_Load': np.int64(0), 'Maximum_Load': np.int64(1), 'Medium_Load': np.int64(2)}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T04:33:32.188679Z",
     "start_time": "2025-10-13T04:33:32.176229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Quick overview of loaded data\n",
    "print(\"=== LOADED DATA OVERVIEW ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "target_distribution = df['target'].value_counts().sort_index()\n",
    "print(target_distribution)\n",
    "print(f\"\\nClass balance:\")\n",
    "for class_val, count in target_distribution.items():\n",
    "    print(f\"  Class {class_val}: {count:,} samples ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for data quality\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Infinite values: {np.isinf(df.select_dtypes(include=[np.number])).sum().sum()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOADED DATA OVERVIEW ===\n",
      "Shape: (35040, 38)\n",
      "Features: 37\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "0    18072\n",
      "1     7272\n",
      "2     9696\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class balance:\n",
      "  Class 0: 18,072 samples (51.6%)\n",
      "  Class 1: 7,272 samples (20.8%)\n",
      "  Class 2: 9,696 samples (27.7%)\n",
      "\n",
      "Data quality check:\n",
      "Missing values: 0\n",
      "Infinite values: 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Prepare Data for Modeling"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T04:33:32.210537Z",
     "start_time": "2025-10-13T04:33:32.194344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare features and target\n",
    "print(\"=== PREPARING DATA FOR MODELING ===\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df[feature_names].copy()\n",
    "y = df['target'].copy()\n",
    "\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "print(f\"Target classes: {sorted(y.unique())}\")\n",
    "\n",
    "# Check for any remaining data quality issues\n",
    "print(f\"\\nFinal data quality check:\")\n",
    "print(f\"Features - Missing values: {X.isnull().sum().sum()}\")\n",
    "# print(f\"Features - Infinite values: {np.isinf(X).sum().sum()}\")\n",
    "print(f\"Target - Missing values: {y.isnull().sum()}\")\n",
    "\n",
    "# Handle any remaining issues\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(\"⚠️ Filling remaining missing values in features...\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "# if np.isinf(X).sum().sum() > 0:\n",
    "#     print(\"⚠️ Replacing infinite values in features...\")\n",
    "#     X = X.replace([np.inf, -np.inf], [X.max().max(), X.min().min()])\n",
    "\n",
    "print(\"✅ Data is ready for modeling!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARING DATA FOR MODELING ===\n",
      "Features (X): (35040, 37)\n",
      "Target (y): (35040,)\n",
      "Target classes: [np.int64(0), np.int64(1), np.int64(2)]\n",
      "\n",
      "Final data quality check:\n",
      "Features - Missing values: 0\n",
      "Target - Missing values: 0\n",
      "✅ Data is ready for modeling!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Data Splitting"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T04:33:32.257134Z",
     "start_time": "2025-10-13T04:33:32.220048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "print(\"=== DATA SPLITTING ===\")\n",
    "\n",
    "# Split with stratification to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Check class balance in splits\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "train_dist = y_train.value_counts().sort_index()\n",
    "for class_val, count in train_dist.items():\n",
    "    print(f\"  Class {class_val}: {count:,} samples ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass distribution in testing set:\")\n",
    "test_dist = y_test.value_counts().sort_index()\n",
    "for class_val, count in test_dist.items():\n",
    "    print(f\"  Class {class_val}: {count:,} samples ({count/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Further split training data for validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter validation split:\")\n",
    "print(f\"Training set: {X_train_split.shape[0]:,} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SPLITTING ===\n",
      "Training set: 28,032 samples\n",
      "Testing set: 7,008 samples\n",
      "Features: 37\n",
      "\n",
      "Class distribution in training set:\n",
      "  Class 0: 14,457 samples (51.6%)\n",
      "  Class 1: 5,818 samples (20.8%)\n",
      "  Class 2: 7,757 samples (27.7%)\n",
      "\n",
      "Class distribution in testing set:\n",
      "  Class 0: 3,615 samples (51.6%)\n",
      "  Class 1: 1,454 samples (20.7%)\n",
      "  Class 2: 1,939 samples (27.7%)\n",
      "\n",
      "After validation split:\n",
      "Training set: 22,425 samples\n",
      "Validation set: 5,607 samples\n",
      "Test set: 7,008 samples\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Model Definition and Initial Training"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T04:33:32.271663Z",
     "start_time": "2025-10-13T04:33:32.266345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define models to train\n",
    "print(\"=== MODEL DEFINITION AND INITIAL TRAINING ===\")\n",
    "\n",
    "# Initialize models with default parameters\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=42, n_jobs=-1),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Neural Network': MLPClassifier(random_state=42, max_iter=500),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Add XGBoost and LightGBM if available\n",
    "if XGBOOST_AVAILABLE:\n",
    "    models['XGBoost'] = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "    print(\"✓ XGBoost added to model list\")\n",
    "\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    models['LightGBM'] = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "    print(\"✓ LightGBM added to model list\")\n",
    "\n",
    "print(f\"\\nTotal models to train: {len(models)}\")\n",
    "print(f\"Models: {list(models.keys())}\")\n",
    "\n",
    "# Initialize results storage\n",
    "model_results = []\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\\n=== TRAINING MODELS WITH DEFAULT PARAMETERS ===\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL DEFINITION AND INITIAL TRAINING ===\n",
      "\n",
      "Total models to train: 10\n",
      "Models: ['Random Forest', 'Gradient Boosting', 'Extra Trees', 'Logistic Regression', 'Decision Tree', 'SVM', 'K-Nearest Neighbors', 'Naive Bayes', 'Neural Network', 'AdaBoost']\n",
      "\n",
      "=== TRAINING MODELS WITH DEFAULT PARAMETERS ===\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T04:33:32.353519Z",
     "start_time": "2025-10-13T04:33:32.278554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train all models and collect initial results\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.fit(X_train_split, y_train_split)\n",
    "\n",
    "        # Make predictions on validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_val_pred_proba = model.predict_proba(X_val) if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        precision = precision_score(y_val, y_val_pred, average='weighted')\n",
    "        recall = recall_score(y_val, y_val_pred, average='weighted')\n",
    "        f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        cv_mean = cv_scores.mean()\n",
    "        cv_std = cv_scores.std()\n",
    "\n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "        # Store results\n",
    "        result = {\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1_Score': f1,\n",
    "            'CV_Mean': cv_mean,\n",
    "            'CV_Std': cv_std,\n",
    "            'Training_Time': training_time\n",
    "        }\n",
    "\n",
    "        model_results.append(result)\n",
    "        trained_models[name] = model\n",
    "\n",
    "        print(f\"  ✓ Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  ✓ F1-Score: {f1:.4f}\")\n",
    "        print(f\"  ✓ CV Score: {cv_mean:.4f} (±{cv_std:.4f})\")\n",
    "        print(f\"  ✓ Training time: {training_time:.2f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error training {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Successfully trained {len(trained_models)} models\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest...\n",
      "  ❌ Error training Random Forest: could not convert string to float: 'Maximum_Load'\n",
      "\n",
      "Training Gradient Boosting...\n",
      "  ❌ Error training Gradient Boosting: could not convert string to float: 'Maximum_Load'\n",
      "\n",
      "Training Extra Trees...\n",
      "  ❌ Error training Extra Trees: could not convert string to float: 'Maximum_Load'\n",
      "\n",
      "Training Logistic Regression...\n",
      "  ❌ Error training Logistic Regression: could not convert string to float: 'Maximum_Load'\n",
      "\n",
      "Training Decision Tree...\n",
      "  ❌ Error training Decision Tree: could not convert string to float: 'Maximum_Load'\n",
      "\n",
      "Training SVM...\n",
      "  ❌ Error training SVM: could not convert string to float: 'Maximum_Load'\n",
      "\n",
      "Training K-Nearest Neighbors...\n",
      "  ❌ Error training K-Nearest Neighbors: could not convert string to float: 'Maximum_Load'\n",
      "\n",
      "Training Naive Bayes...\n",
      "  ❌ Error training Naive Bayes: could not convert string to float: 'Maximum_Load'\n",
      "\n",
      "Training Neural Network...\n",
      "  ❌ Error training Neural Network: could not convert string to float: 'Maximum_Load'\n",
      "\n",
      "Training AdaBoost...\n",
      "  ❌ Error training AdaBoost: could not convert string to float: 'Maximum_Load'\n",
      "\n",
      "✅ Successfully trained 0 models\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Model Comparison and Selection"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T04:33:32.677381Z",
     "start_time": "2025-10-13T04:33:32.357538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create comprehensive results comparison\n",
    "print(\"=== MODEL COMPARISON AND SELECTION ===\")\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nModel Performance Comparison (sorted by Accuracy):\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 0].barh(results_df['Model'], results_df['Accuracy'], color='lightblue')\n",
    "axes[0, 0].set_xlabel('Accuracy')\n",
    "axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[0, 1].barh(results_df['Model'], results_df['F1_Score'], color='lightcoral')\n",
    "axes[0, 1].set_xlabel('F1-Score')\n",
    "axes[0, 1].set_title('Model F1-Score Comparison')\n",
    "axes[0, 1].set_xlim(0, 1)\n",
    "\n",
    "# Cross-validation scores\n",
    "axes[1, 0].barh(results_df['Model'], results_df['CV_Mean'], color='lightgreen')\n",
    "axes[1, 0].set_xlabel('CV Mean Accuracy')\n",
    "axes[1, 0].set_title('Cross-Validation Score Comparison')\n",
    "axes[1, 0].set_xlim(0, 1)\n",
    "\n",
    "# Training time\n",
    "axes[1, 1].barh(results_df['Model'], results_df['Training_Time'], color='lightyellow')\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)')\n",
    "axes[1, 1].set_title('Training Time Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select top performing models\n",
    "top_n = min(3, len(results_df))\n",
    "top_models = results_df.head(top_n)['Model'].tolist()\n",
    "\n",
    "print(f\"\\n🏆 Top {top_n} performing models:\")\n",
    "for i, model_name in enumerate(top_models, 1):\n",
    "    model_row = results_df[results_df['Model'] == model_name].iloc[0]\n",
    "    print(f\"{i}. {model_name}:\")\n",
    "    print(f\"   • Accuracy: {model_row['Accuracy']:.4f}\")\n",
    "    print(f\"   • F1-Score: {model_row['F1_Score']:.4f}\")\n",
    "    print(f\"   • CV Score: {model_row['CV_Mean']:.4f} (±{model_row['CV_Std']:.4f})\")\n",
    "\n",
    "best_model_name = top_models[0]\n",
    "best_model = trained_models[best_model_name]\n",
    "print(f\"\\n🥇 Best performing model: {best_model_name}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL COMPARISON AND SELECTION ===\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[32m~\\AppData\\Local\\Temp\\ipykernel_15456\\289773260.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      2\u001B[39m print(\u001B[33m\"=== MODEL COMPARISON AND SELECTION ===\"\u001B[39m)\n\u001B[32m      3\u001B[39m \n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Convert results to DataFrame for better visualization\u001B[39;00m\n\u001B[32m      5\u001B[39m results_df = pd.DataFrame(model_results)\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m results_df = results_df.sort_values(\u001B[33m'Accuracy'\u001B[39m, ascending=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m      7\u001B[39m \n\u001B[32m      8\u001B[39m print(\u001B[33m\"\\nModel Performance Comparison (sorted by Accuracy):\"\u001B[39m)\n\u001B[32m      9\u001B[39m print(results_df.round(\u001B[32m4\u001B[39m))\n",
      "\u001B[32mD:\\Codes\\AI\\Project\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001B[39m\n\u001B[32m   7192\u001B[39m             )\n\u001B[32m   7193\u001B[39m         \u001B[38;5;28;01melif\u001B[39;00m len(by):\n\u001B[32m   7194\u001B[39m             \u001B[38;5;66;03m# len(by) == 1\u001B[39;00m\n\u001B[32m   7195\u001B[39m \n\u001B[32m-> \u001B[39m\u001B[32m7196\u001B[39m             k = self._get_label_or_level_values(by[\u001B[32m0\u001B[39m], axis=axis)\n\u001B[32m   7197\u001B[39m \n\u001B[32m   7198\u001B[39m             \u001B[38;5;66;03m# need to rewrap column in Series to apply key function\u001B[39;00m\n\u001B[32m   7199\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[32mD:\\Codes\\AI\\Project\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, key, axis)\u001B[39m\n\u001B[32m   1907\u001B[39m             values = self.xs(key, axis=other_axes[\u001B[32m0\u001B[39m])._values\n\u001B[32m   1908\u001B[39m         \u001B[38;5;28;01melif\u001B[39;00m self._is_level_reference(key, axis=axis):\n\u001B[32m   1909\u001B[39m             values = self.axes[axis].get_level_values(key)._values\n\u001B[32m   1910\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1911\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m KeyError(key)\n\u001B[32m   1912\u001B[39m \n\u001B[32m   1913\u001B[39m         \u001B[38;5;66;03m# Check for duplicates\u001B[39;00m\n\u001B[32m   1914\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m values.ndim > \u001B[32m1\u001B[39m:\n",
      "\u001B[31mKeyError\u001B[39m: 'Accuracy'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Hyperparameter Tuning for Best Models"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Hyperparameter tuning for top performing models\n",
    "print(\"=== HYPERPARAMETER TUNING ===\")\n",
    "\n",
    "# Define parameter grids for top models\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'solver': ['liblinear', 'lbfgs'],\n",
    "        'penalty': ['l1', 'l2']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    param_grids['XGBoost'] = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    param_grids['LightGBM'] = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "# Perform hyperparameter tuning for top models\n",
    "tuned_models = {}\n",
    "tuning_results = []\n",
    "\n",
    "for model_name in top_models[:2]:  # Tune top 2 models to save time\n",
    "    if model_name in param_grids:\n",
    "        print(f\"\\nTuning hyperparameters for {model_name}...\")\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        try:\n",
    "            # Get base model\n",
    "            base_model = trained_models[model_name]\n",
    "\n",
    "            # Setup GridSearch with cross-validation\n",
    "            grid_search = GridSearchCV(\n",
    "                base_model,\n",
    "                param_grids[model_name],\n",
    "                cv=3,  # Use 3-fold CV to save time\n",
    "                scoring='accuracy',\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Fit grid search\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            # Get best model\n",
    "            best_tuned_model = grid_search.best_estimator_\n",
    "            tuned_models[model_name] = best_tuned_model\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            y_val_pred_tuned = best_tuned_model.predict(X_val)\n",
    "            tuned_accuracy = accuracy_score(y_val, y_val_pred_tuned)\n",
    "            tuned_f1 = f1_score(y_val, y_val_pred_tuned, average='weighted')\n",
    "\n",
    "            tuning_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "            # Compare with original\n",
    "            original_accuracy = results_df[results_df['Model'] == model_name]['Accuracy'].iloc[0]\n",
    "            improvement = tuned_accuracy - original_accuracy\n",
    "\n",
    "            result = {\n",
    "                'Model': model_name,\n",
    "                'Original_Accuracy': original_accuracy,\n",
    "                'Tuned_Accuracy': tuned_accuracy,\n",
    "                'Improvement': improvement,\n",
    "                'Tuned_F1': tuned_f1,\n",
    "                'Best_Params': grid_search.best_params_,\n",
    "                'Tuning_Time': tuning_time\n",
    "            }\n",
    "\n",
    "            tuning_results.append(result)\n",
    "\n",
    "            print(f\"  ✓ Original accuracy: {original_accuracy:.4f}\")\n",
    "            print(f\"  ✓ Tuned accuracy: {tuned_accuracy:.4f}\")\n",
    "            print(f\"  ✓ Improvement: {improvement:.4f}\")\n",
    "            print(f\"  ✓ Best parameters: {grid_search.best_params_}\")\n",
    "            print(f\"  ✓ Tuning time: {tuning_time:.2f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error tuning {model_name}: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"\\nSkipping hyperparameter tuning for {model_name} (no parameter grid defined)\")\n",
    "        tuned_models[model_name] = trained_models[model_name]\n",
    "\n",
    "print(f\"\\n✅ Hyperparameter tuning completed for {len(tuning_results)} models\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Final Model Evaluation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select the best model (tuned if available, otherwise original)\n",
    "print(\"=== FINAL MODEL EVALUATION ===\")\n",
    "\n",
    "# Determine the final best model\n",
    "if tuning_results:\n",
    "    # Find best tuned model\n",
    "    best_tuned = max(tuning_results, key=lambda x: x['Tuned_Accuracy'])\n",
    "    final_best_model_name = best_tuned['Model']\n",
    "    final_best_model = tuned_models[final_best_model_name]\n",
    "    print(f\"Selected tuned model: {final_best_model_name}\")\n",
    "    print(f\"Tuned accuracy: {best_tuned['Tuned_Accuracy']:.4f}\")\n",
    "else:\n",
    "    # Use original best model\n",
    "    final_best_model_name = best_model_name\n",
    "    final_best_model = best_model\n",
    "    print(f\"Selected original model: {final_best_model_name}\")\n",
    "\n",
    "# Retrain on full training data and evaluate on test set\n",
    "print(f\"\\nRetraining {final_best_model_name} on full training data...\")\n",
    "final_best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_test_pred = final_best_model.predict(X_test)\n",
    "y_test_pred_proba = final_best_model.predict_proba(X_test)\n",
    "\n",
    "# Calculate final metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(f\"\\n🎯 FINAL MODEL PERFORMANCE ON TEST SET:\")\n",
    "print(f\"Model: {final_best_model_name}\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\n📊 DETAILED CLASSIFICATION REPORT:\")\n",
    "class_names = ['Light_Load', 'Medium_Load', 'Maximum_Load']  # Adjust based on your classes\n",
    "try:\n",
    "    target_classes = transformation_metadata['target_classes']\n",
    "    class_names = [k for k, v in sorted(target_classes.items(), key=lambda x: x[1])]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "report = classification_report(y_test, y_test_pred, target_names=class_names)\n",
    "print(report)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Confusion Matrix Visualization\n",
    "print(\"\\n=== CONFUSION MATRIX ===\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names)\n",
    "plt.title(f'Confusion Matrix - {final_best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class metrics\n",
    "print(\"\\nPer-class performance:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = (y_test == i)\n",
    "    class_predictions = (y_test_pred == i)\n",
    "\n",
    "    tp = np.sum(class_mask & class_predictions)\n",
    "    fp = np.sum(~class_mask & class_predictions)\n",
    "    fn = np.sum(class_mask & ~class_predictions)\n",
    "    tn = np.sum(~class_mask & ~class_predictions)\n",
    "\n",
    "    class_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    class_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    class_f1 = 2 * (class_precision * class_recall) / (class_precision + class_recall) if (class_precision + class_recall) > 0 else 0\n",
    "\n",
    "    print(f\"\\n{class_name} (Class {i}):\")\n",
    "    print(f\"  Precision: {class_precision:.4f}\")\n",
    "    print(f\"  Recall: {class_recall:.4f}\")\n",
    "    print(f\"  F1-Score: {class_f1:.4f}\")\n",
    "    print(f\"  Support: {np.sum(class_mask)}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Feature Importance Analysis"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Feature importance analysis for the final model\n",
    "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "if hasattr(final_best_model, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    feature_importance = final_best_model.feature_importances_\n",
    "\n",
    "    # Create DataFrame for better handling\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(f\"\\nTop 15 most important features for {final_best_model_name}:\")\n",
    "    print(feature_importance_df.head(15))\n",
    "\n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_df.head(20)\n",
    "\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 20 Feature Importances - {final_best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save feature importance\n",
    "    feature_importance_df.to_csv('feature_importance.csv', index=False)\n",
    "    print(\"\\n✓ Feature importance saved to feature_importance.csv\")\n",
    "\n",
    "elif hasattr(final_best_model, 'coef_'):\n",
    "    # For linear models\n",
    "    feature_coefficients = np.abs(final_best_model.coef_).mean(axis=0)\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient_Magnitude': feature_coefficients\n",
    "    }).sort_values('Coefficient_Magnitude', ascending=False)\n",
    "\n",
    "    print(f\"\\nTop 15 features by coefficient magnitude for {final_best_model_name}:\")\n",
    "    print(feature_importance_df.head(15))\n",
    "\n",
    "else:\n",
    "    print(f\"\\n⚠️ {final_best_model_name} does not provide feature importance information\")\n",
    "    feature_importance_df = None"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Model Prediction Examples"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Show some prediction examples\n",
    "print(\"=== MODEL PREDICTION EXAMPLES ===\")\n",
    "\n",
    "# Select a few test samples for demonstration\n",
    "sample_indices = np.random.choice(X_test.index, size=min(10, len(X_test)), replace=False)\n",
    "sample_X = X_test.loc[sample_indices]\n",
    "sample_y_true = y_test.loc[sample_indices]\n",
    "sample_y_pred = final_best_model.predict(sample_X)\n",
    "sample_y_pred_proba = final_best_model.predict_proba(sample_X)\n",
    "\n",
    "print(f\"\\nPrediction examples from test set:\")\n",
    "print(\"\" + \"=\"*80 + \"\")\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    true_class = sample_y_true.loc[idx]\n",
    "    pred_class = sample_y_pred[i]\n",
    "    probabilities = sample_y_pred_proba[i]\n",
    "\n",
    "    true_class_name = class_names[true_class] if true_class < len(class_names) else f\"Class_{true_class}\"\n",
    "    pred_class_name = class_names[pred_class] if pred_class < len(class_names) else f\"Class_{pred_class}\"\n",
    "\n",
    "    print(f\"\\nSample {i+1} (Index: {idx}):\")\n",
    "    print(f\"  True Class: {true_class_name}\")\n",
    "    print(f\"  Predicted Class: {pred_class_name}\")\n",
    "    print(f\"  Prediction Confidence: {probabilities[pred_class]:.4f}\")\n",
    "    print(f\"  All Probabilities: {[f'{class_names[j]}: {prob:.4f}' for j, prob in enumerate(probabilities)]}\")\n",
    "\n",
    "    if true_class == pred_class:\n",
    "        print(f\"  ✅ Correct Prediction\")\n",
    "    else:\n",
    "        print(f\"  ❌ Incorrect Prediction\")\n",
    "\n",
    "print(\"\" + \"=\"*80 + \"\")\n",
    "\n",
    "# Summary statistics\n",
    "correct_predictions = np.sum(sample_y_true == sample_y_pred)\n",
    "print(f\"\\nSample accuracy: {correct_predictions}/{len(sample_indices)} ({correct_predictions/len(sample_indices)*100:.1f}%)\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10. Save Final Model and Results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the final trained model and results\n",
    "print(\"=== SAVING FINAL MODEL AND RESULTS ===\")\n",
    "\n",
    "try:\n",
    "    # Save the final model\n",
    "    model_filename = f'best_model_{final_best_model_name.replace(\" \", \"_\").lower()}.pkl'\n",
    "    joblib.dump(final_best_model, model_filename)\n",
    "    print(f\"✓ Final model saved as: {model_filename}\")\n",
    "\n",
    "    # Save model metadata\n",
    "    model_metadata = {\n",
    "        'model_name': final_best_model_name,\n",
    "        'model_type': type(final_best_model).__name__,\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1_score': test_f1,\n",
    "        'feature_count': len(feature_names),\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'class_names': class_names,\n",
    "        'target_classes': transformation_metadata.get('target_classes', {}),\n",
    "        'model_parameters': final_best_model.get_params() if hasattr(final_best_model, 'get_params') else {}\n",
    "    }\n",
    "\n",
    "    joblib.dump(model_metadata, 'model_metadata.pkl')\n",
    "    print(\"✓ Model metadata saved as: model_metadata.pkl\")\n",
    "\n",
    "    # Save comprehensive results\n",
    "    comprehensive_results = {\n",
    "        'all_model_results': results_df,\n",
    "        'tuning_results': tuning_results,\n",
    "        'final_model_performance': {\n",
    "            'accuracy': test_accuracy,\n",
    "            'precision': test_precision,\n",
    "            'recall': test_recall,\n",
    "            'f1_score': test_f1\n",
    "        },\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': classification_report(y_test, y_test_pred,\n",
    "                                                     target_names=class_names,\n",
    "                                                     output_dict=True),\n",
    "        'feature_importance': feature_importance_df.to_dict() if feature_importance_df is not None else None\n",
    "    }\n",
    "\n",
    "    joblib.dump(comprehensive_results, 'comprehensive_results.pkl')\n",
    "    print(\"✓ Comprehensive results saved as: comprehensive_results.pkl\")\n",
    "\n",
    "    # Save predictions for analysis\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'y_true': y_test,\n",
    "        'y_pred': y_test_pred,\n",
    "        'correct': (y_test == y_test_pred)\n",
    "    })\n",
    "\n",
    "    # Add probability columns\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        predictions_df[f'prob_{class_name}'] = y_test_pred_proba[:, i]\n",
    "\n",
    "    predictions_df.to_csv('test_predictions.csv', index=False)\n",
    "    print(\"✓ Test predictions saved as: test_predictions.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving model and results: {e}\")\n",
    "\n",
    "print(\"\\n📁 Saved files:\")\n",
    "saved_files = [\n",
    "    model_filename,\n",
    "    'model_metadata.pkl',\n",
    "    'comprehensive_results.pkl',\n",
    "    'test_predictions.csv'\n",
    "]\n",
    "\n",
    "if feature_importance_df is not None:\n",
    "    saved_files.append('feature_importance.csv')\n",
    "\n",
    "for file in saved_files:\n",
    "    print(f\"  • {file}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 11. Model Training Summary"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate comprehensive model training summary\n",
    "print(\"\" + \"=\"*60 + \"\")\n",
    "print(\"            MODEL TRAINING SUMMARY\")\n",
    "print(\"\" + \"=\"*60 + \"\")\n",
    "\n",
    "print(f\"📊 Dataset Information:\")\n",
    "print(f\"   • Total samples: {len(df):,}\")\n",
    "print(f\"   • Features: {len(feature_names)}\")\n",
    "print(f\"   • Classes: {len(class_names)} ({', '.join(class_names)})\")\n",
    "print(f\"   • Training samples: {len(X_train):,} ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"   • Test samples: {len(X_test):,} ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🤖 Models Evaluated:\")\n",
    "print(f\"   • Total models trained: {len(results_df)}\")\n",
    "print(f\"   • Models with hyperparameter tuning: {len(tuning_results)}\")\n",
    "print(f\"   • Best performing model: {final_best_model_name}\")\n",
    "\n",
    "print(f\"\\n📈 Model Performance Ranking:\")\n",
    "for i, (_, row) in enumerate(results_df.head(5).iterrows(), 1):\n",
    "    print(f\"   {i}. {row['Model']}: {row['Accuracy']:.4f} accuracy, {row['F1_Score']:.4f} F1\")\n",
    "\n",
    "print(f\"\\n🎯 Final Model Performance:\")\n",
    "print(f\"   • Model: {final_best_model_name}\")\n",
    "print(f\"   • Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"   • Test Precision: {test_precision:.4f}\")\n",
    "print(f\"   • Test Recall: {test_recall:.4f}\")\n",
    "print(f\"   • Test F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n📊 Per-Class Performance:\")\n",
    "class_report_dict = classification_report(y_test, y_test_pred, target_names=class_names, output_dict=True)\n",
    "for class_name in class_names:\n",
    "    if class_name in class_report_dict:\n",
    "        metrics = class_report_dict[class_name]\n",
    "        print(f\"   • {class_name}:\")\n",
    "        print(f\"     - Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"     - Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"     - F1-Score: {metrics['f1-score']:.4f}\")\n",
    "        print(f\"     - Support: {metrics['support']}\")\n",
    "\n",
    "if feature_importance_df is not None:\n",
    "    print(f\"\\n🔝 Top 5 Most Important Features:\")\n",
    "    for i, (_, row) in enumerate(feature_importance_df.head(5).iterrows(), 1):\n",
    "        importance_col = 'Importance' if 'Importance' in row else 'Coefficient_Magnitude'\n",
    "        print(f\"   {i}. {row['Feature']}: {row[importance_col]:.4f}\")\n",
    "\n",
    "print(f\"\\n💾 Saved Artifacts:\")\n",
    "for file in saved_files:\n",
    "    print(f\"   • {file}\")\n",
    "\n",
    "print(f\"\\n⏱️ Training Summary:\")\n",
    "total_training_time = sum([r['Training_Time'] for r in model_results])\n",
    "if tuning_results:\n",
    "    total_tuning_time = sum([r['Tuning_Time'] for r in tuning_results])\n",
    "    print(f\"   • Total training time: {total_training_time:.2f}s\")\n",
    "    print(f\"   • Total tuning time: {total_tuning_time:.2f}s\")\n",
    "    print(f\"   • Total time: {total_training_time + total_tuning_time:.2f}s\")\n",
    "else:\n",
    "    print(f\"   • Total training time: {total_training_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n🎯 Model Readiness:\")\n",
    "if test_accuracy >= 0.9:\n",
    "    readiness = \"🌟 Excellent - Ready for Production\"\n",
    "elif test_accuracy >= 0.8:\n",
    "    readiness = \"✅ Good - Ready for Deployment\"\n",
    "elif test_accuracy >= 0.7:\n",
    "    readiness = \"⚠️ Acceptable - Consider Further Tuning\"\n",
    "else:\n",
    "    readiness = \"❌ Poor - Needs Significant Improvement\"\n",
    "\n",
    "print(f\"   • Status: {readiness}\")\n",
    "print(f\"   • Model file: {model_filename}\")\n",
    "print(f\"   • Ready for inference: ✅\")\n",
    "\n",
    "print(\"\" + \"=\"*60 + \"\")\n",
    "print(\"Model Training Phase Completed Successfully!\")\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "print(f\"🏆 Best Model: {final_best_model_name} with {test_accuracy:.4f} accuracy\")\n",
    "print(\"\" + \"=\"*60 + \"\")\n",
    "\n",
    "print(\"\\n🚀 Next Steps:\")\n",
    "print(\"   1. Load the saved model for inference\")\n",
    "print(\"   2. Monitor model performance in production\")\n",
    "print(\"   3. Retrain periodically with new data\")\n",
    "print(\"   4. Consider ensemble methods for improved performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
